# -*- coding: utf-8 -*-
"""Loan Eligibility.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zL3G8iOzZlrewxq4MmvloRyIxodEzPCG
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df_train= pd.read_csv('/content/train_hl.csv')
df_test= pd.read_csv('/content/test_hl.csv')

df_train.head()

df_train.info()

df_train.isnull().sum()

df_train.shape

#if we drop nan values

#df_train= df_train.dropna()

#if we donot drop nan values rather fill them with mean, median or mode method, (it has given better results).


df_train['Gender'].fillna(df_train['Gender'].mode()[0], inplace=True)
df_train['Married'].fillna(df_train['Married'].mode()[0], inplace=True)
df_train['Dependents'].fillna(df_train['Dependents'].mode()[0], inplace=True)
df_train['Self_Employed'].fillna(df_train['Self_Employed'].mode()[0], inplace=True)
df_train['LoanAmount'].fillna(df_train['LoanAmount'].median(), inplace=True)
df_train['Loan_Amount_Term'].fillna(df_train['Loan_Amount_Term'].median(), inplace=True)
df_train['Credit_History'].fillna(df_train['Credit_History'].mode()[0], inplace=True)

df_train.dtypes

df_train.describe()

sns.heatmap(df_train.corr())

sns.pairplot(df_train , hue='Loan_Status' )

df_train= pd.get_dummies(data= df_train, columns=['Gender'], drop_first= True)

df_train= pd.get_dummies(data= df_train, columns=['Married'], drop_first= True)

df_train= pd.get_dummies(data= df_train, columns=['Education'], drop_first= True)

df_train= pd.get_dummies(data= df_train, columns=['Self_Employed'], drop_first= True)

df_train= pd.get_dummies(data= df_train, columns=['Loan_Status'], drop_first= True)



df_train.head()

df_train= df_train.drop('Loan_ID', axis=1)

df_train.head()

df_train['Dependents'].unique()

df_train['Dependents'].fillna(df_train['Dependents'].mode()[0], inplace=True)

df_train['Dependents'].unique()

df_train= pd.get_dummies(data= df_train, columns=['Dependents'], drop_first= True)

df_train= pd.get_dummies(data= df_train, columns=['Property_Area'], drop_first= True)

df_train.shape

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler



X= df_train.drop('Loan_Status_Y', axis=1)
y= df_train['Loan_Status_Y']

X.shape

y.shape



x_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=0)

sc = StandardScaler()
X_train = sc.fit_transform(x_train)
X_test = sc.transform(x_test)

# from sklearn.impute import SimpleImputer
# imp = SimpleImputer(strategy='mean')
# imp_train = imp.fit(x_train)
# X_train = imp_train.transform(x_train)
# X_test_imp = imp_train.transform(x_test)

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from xgboost.sklearn import XGBClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier

clf = LogisticRegression(max_iter=1000,random_state=0)
clf.fit(x_train,y_train)

y_pred= clf.predict(x_test)

accuracy= accuracy_score(y_test, y_pred)

accuracy

print(confusion_matrix(y_pred,y_test))

print(classification_report(y_pred,y_test))

knn = KNeighborsClassifier(n_neighbors=50)
knn.fit(x_train,y_train)

y_pred2= knn.predict(x_test)

accuracy= accuracy_score(y_test, y_pred2)
accuracy

print(confusion_matrix(y_pred2,y_test))

print(classification_report(y_pred2,y_test))

rfc= RandomForestClassifier(n_estimators=100,max_depth=3,min_samples_leaf = 10)
rfc.fit(x_train, y_train)

y_pred3= rfc.predict(x_test)

accuracy= accuracy_score(y_test, y_pred3)
accuracy

print(confusion_matrix(y_pred3,y_test))

print(classification_report(y_pred3,y_test))

svc= SVC()
svc.fit(x_train, y_train)

y_pred4= svc.predict(x_test)

accuracy= accuracy_score(y_test, y_pred4)
accuracy

print(confusion_matrix(y_pred4,y_test))

print(classification_report(y_pred4,y_test))

dtc= DecisionTreeClassifier(criterion='gini',splitter='random',max_leaf_nodes=8,min_samples_leaf=10,max_depth=5)
#dtc= DecisionTreeClassifier(max_depth=3,min_samples_leaf = 35)
dtc.fit(x_train, y_train)

y_pred5= dtc.predict(x_test)

accuracy= accuracy_score(y_test, y_pred5)
accuracy

print(confusion_matrix(y_pred5,y_test))

print(classification_report(y_pred5,y_test))

from xgboost import XGBClassifier

xgb_model = XGBClassifier(n_estimators=50,max_depth=8, max_leaves=12,n_jobs=100)

xgb_model.fit(x_train,y_train)

y_pred6=xgb_model.predict(x_test)

accuracy= accuracy_score(y_test, y_pred6)
accuracy

print(confusion_matrix(y_pred6,y_test))

print(classification_report(y_pred6,y_test))

